{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains code of previous used dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fa6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "DATA_FILE = \"/home/karthikssalian/work/RWKV-PEFT/revision/para-nmt-50m.txt\"\n",
    "TARGET_FILE = \"/home/karthikssalian/work/RWKV-PEFT/revision/converted.jsonl\"\n",
    "\n",
    "\n",
    "def simplify_to_keywords(text):\n",
    "    \"\"\"\n",
    "    Convert sentence into a keyword list with inline modifiers.\n",
    "    - Removes stopwords\n",
    "    - Keeps content words\n",
    "    - Inserts 'question' or 'exclamation' where '?' or '!' appear inline\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    tokens = re.findall(r\"[A-Za-z']+|[!?]\", text)\n",
    "    keywords = []\n",
    "\n",
    "    for token in tokens:\n",
    "        t = token.lower()\n",
    "        if t in STOP:\n",
    "            continue\n",
    "        if t == \"?\":\n",
    "            keywords.append(\"question\")\n",
    "        elif t == \"!\":\n",
    "            keywords.append(\"exclamation\")\n",
    "        else:\n",
    "            keywords.append(t)\n",
    "\n",
    "    if not keywords:\n",
    "        # fallback if all words removed\n",
    "        keywords = [t.lower() for t in re.findall(r\"[A-Za-z']+\", text.lower())[:3]]\n",
    "\n",
    "    return \" \".join(keywords)\n",
    "\n",
    "\n",
    "# clear the output file first\n",
    "with open(TARGET_FILE, \"w\", encoding=\"utf-8\"):\n",
    "    pass\n",
    "\n",
    "total = 0\n",
    "with open(TARGET_FILE, \"a\", encoding=\"utf-8\") as write_file:\n",
    "    with open(DATA_FILE, encoding=\"utf-8\") as read_file:\n",
    "        for line in read_file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            s1, s2 = parts[0], parts[1]\n",
    "\n",
    "            for sent in (s1, s2):\n",
    "                kw = simplify_to_keywords(sent)\n",
    "                text = f\"User: {kw}\\n\\nAssistant: {sent}\"\n",
    "                write_file.write(json.dumps({\"text\": text}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                total += 1\n",
    "                if total % 5000 == 0:\n",
    "                    print(f\"Processed {total:,} datapoints\", end=\"\\r\")\n",
    "\n",
    "print(f\"\\n✅ Created {total:,} datapoints → {TARGET_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
